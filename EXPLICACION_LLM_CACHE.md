# ğŸ’¡ ExplicaciÃ³n: LLM Cache

## Â¿QuÃ© es LLM Cache?

**Cache de respuestas del LLM** para evitar llamadas innecesarias y costosas.

---

## ğŸ¯ Concepto

### **Problema sin cache:**

```typescript
// Cada vez que un vehÃ­culo tiene temperatura alta
Temperature: 105Â°C, Speed: 80 km/h, RPM: 2500
  â†“
LLM analiza â†’ $0.00011
  â†“
Response: "Motor sobrecalentado, reducir velocidad..."

// 10 segundos despuÃ©s, MISMO vehÃ­culo, MISMAS condiciones
Temperature: 105Â°C, Speed: 80 km/h, RPM: 2500
  â†“
LLM analiza DE NUEVO â†’ $0.00011  âŒ Desperdicio
  â†“
Response: "Motor sobrecalentado, reducir velocidad..." (igual)
```

**Problema:** Pagamos por el mismo anÃ¡lisis mÃºltiples veces.

---

### **SoluciÃ³n con cache:**

```typescript
// Primera vez
Temperature: 105Â°C, Speed: 80 km/h, RPM: 2500
  â†“
Cache key: "temperature-105-80"
  â†“
No hay cache, llamar LLM â†’ $0.00011
  â†“
Guardar respuesta en cache
  â†“
Response: "Motor sobrecalentado..."

// 10 segundos despuÃ©s, MISMAS condiciones
Temperature: 105Â°C, Speed: 80 km/h, RPM: 2500
  â†“
Cache key: "temperature-105-80"
  â†“
Â¡Existe en cache! â†’ $0 âœ…
  â†“
Response: "Motor sobrecalentado..." (desde cache)
```

---

## ğŸ“¦ ImplementaciÃ³n Simple

```typescript
// lib/llm-cache.ts

// Cache simple en memoria (Map)
const cache = new Map<string, {
  alert: Alert;
  timestamp: number;
}>();

export function getCachedAnalysis(cacheKey: string): Alert | null {
  const cached = cache.get(cacheKey);

  // Cache vÃ¡lido por 5 minutos
  if (cached && Date.now() - cached.timestamp < 5 * 60 * 1000) {
    return cached.alert;
  }

  // Cache expirÃ³ o no existe
  return null;
}

export function setCachedAnalysis(cacheKey: string, alert: Alert) {
  cache.set(cacheKey, {
    alert,
    timestamp: Date.now()
  });
}
```

---

## ğŸ”‘ Cache Key Strategy

El "cache key" determina cuÃ¡ndo dos situaciones son "iguales":

### **Estrategia 1: Muy especÃ­fica (poco reuso)**
```typescript
const cacheKey = `${anomalyType}-${data.engine_temp}-${data.speed}-${data.rpm}-${data.battery_voltage}`;
// Ejemplo: "temperature-105-80-2500-13.5"
// Problema: Cambio de 1Â°C = nueva llamada LLM
```

### **Estrategia 2: Redondeada (mÃ¡s reuso)** âœ…
```typescript
const cacheKey = `${anomalyType}-${Math.round(data.engine_temp / 5) * 5}-${Math.round(data.speed / 10) * 10}`;
// Ejemplo: "temperature-105-80"
// 103Â°C â†’ 105, 104Â°C â†’ 105, 106Â°C â†’ 105 (mismo cache)
// 82 km/h â†’ 80, 79 km/h â†’ 80 (mismo cache)
```

### **Estrategia 3: Por rangos (mÃ¡ximo reuso)**
```typescript
function getTemperatureRange(temp: number): string {
  if (temp < 90) return 'normal';
  if (temp < 100) return 'warm';
  if (temp < 110) return 'hot';
  return 'critical';
}

const cacheKey = `${anomalyType}-${getTemperatureRange(data.engine_temp)}`;
// Ejemplo: "temperature-hot"
// Cualquier temp 100-109Â°C usa el mismo cache
```

---

## ğŸ’° Impacto en Costos

### **Sin cache:**
```
5 vehÃ­culos Ã— 12 lecturas/minuto Ã— 30% anomalÃ­as = 18 llamadas/min
18 Ã— 60 = 1,080 llamadas/hora
1,080 Ã— $0.00011 = $0.12/hora

8 horas/dÃ­a Ã— 30 dÃ­as = $28.80/mes
```

### **Con cache 50% (estrategia 2):**
```
1,080 llamadas/hora Ã— 50% = 540 llamadas reales
540 Ã— $0.00011 = $0.06/hora

8 horas/dÃ­a Ã— 30 dÃ­as = $14.40/mes  âœ… 50% ahorro
```

### **Con cache 70% (estrategia 3):**
```
1,080 llamadas/hora Ã— 30% = 324 llamadas reales
324 Ã— $0.00011 = $0.036/hora

8 horas/dÃ­a Ã— 30 dÃ­as = $8.64/mes  âœ… 70% ahorro
```

---

## ğŸ§ª Ejemplo de Uso

```typescript
// lib/iot-simulator.ts

import { getCachedAnalysis, setCachedAnalysis } from './llm-cache';
import { analyzeVehicleCondition } from './llm-analyzer';

async generateAlert(data: VehicleIoTData): Promise<Alert | null> {
  // 1. Detectar anomalÃ­a
  let anomalyType: string | null = null;

  if (data.engine_temp && data.engine_temp > 100) {
    anomalyType = 'temperature';
  }
  // ... otros checks

  if (!anomalyType) return null;

  // 2. Generar cache key
  const cacheKey = `${anomalyType}-${Math.round(data.engine_temp / 5) * 5}-${Math.round(data.speed / 10) * 10}`;

  // 3. Verificar cache
  const cached = getCachedAnalysis(cacheKey);
  if (cached) {
    console.log(`[LLM Cache] HIT for ${cacheKey}`);
    return cached;
  }

  // 4. No hay cache, llamar LLM
  console.log(`[LLM Cache] MISS for ${cacheKey}, calling LLM...`);
  const analysis = await analyzeVehicleCondition(data, anomalyType);

  // 5. Guardar en cache
  setCachedAnalysis(cacheKey, analysis);

  return analysis;
}
```

---

## ğŸ“Š Logs Ejemplo

```bash
[LLM Cache] MISS for temperature-105-80, calling LLM...
[LLM] Analyzing vehicle BUS-001 for temperature anomaly...
[LLM] Response: { severity: 'high', description: '...' }
[LLM Cache] Saved cache for temperature-105-80

# 10 segundos despuÃ©s, mismo vehÃ­culo
[LLM Cache] HIT for temperature-105-80
[SimulationManager] Alert created: engine_overheating for BUS-001

# Otro vehÃ­culo, condiciones similares
[LLM Cache] HIT for temperature-105-80  âœ… Reuso!
[SimulationManager] Alert created: engine_overheating for BUS-003
```

---

## âš¡ Optimizaciones Avanzadas

### **1. Limpieza de cache viejo**
```typescript
// Ejecutar cada 10 minutos
setInterval(() => {
  const now = Date.now();
  for (const [key, value] of cache.entries()) {
    if (now - value.timestamp > 10 * 60 * 1000) {
      cache.delete(key);
    }
  }
}, 10 * 60 * 1000);
```

### **2. Cache persistente (Redis)**
Para producciÃ³n, usar Redis en lugar de Map:
```typescript
import { Redis } from '@upstash/redis';

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_URL,
  token: process.env.UPSTASH_REDIS_TOKEN
});

export async function getCachedAnalysis(key: string) {
  return await redis.get(key);
}

export async function setCachedAnalysis(key: string, value: Alert) {
  await redis.setex(key, 300, JSON.stringify(value)); // 5 min TTL
}
```

### **3. Cache por tenant**
```typescript
const cacheKey = `${tenantId}:${anomalyType}-${temp}-${speed}`;
// Aislar cache por organizaciÃ³n
```

---

## ğŸ¯ RecomendaciÃ³n para tu MVP

**Usar Estrategia 2 (Redondeada)** con cache simple en memoria:

```typescript
âœ… Implementar:
- Cache en Map (lib/llm-cache.ts)
- Cache key redondeado: temp/5, speed/10
- TTL: 5 minutos
- Cleanup cada 10 minutos

âŒ NO necesitas (por ahora):
- Redis (complejidad extra)
- Cache persistente
- Analytics de cache hit rate
```

**Resultado esperado:**
- 50-60% de cache hit rate
- ReducciÃ³n de costos a $10-15/mes
- Sin complejidad adicional

---

**Â¿Tiene sentido?** Es bÃ¡sicamente un "diccionario" que recuerda respuestas del LLM para no pagar dos veces por lo mismo.
