# ü§ñ Fase 1: Configuraci√≥n de Alertas Inteligentes con LLM

## üìã Resumen

La Fase 1 implementa un sistema de diagn√≥stico inteligente usando OpenAI (gpt-4o-mini) como "mec√°nico experto" que analiza datos de veh√≠culos y genera diagn√≥sticos profesionales con recomendaciones accionables.

## ‚úÖ Lo que se implement√≥

### 1. Servicio OpenAI (`lib/openai-service.ts`)
- ‚úÖ Cliente OpenAI configurado
- ‚úÖ Rate limiting (50 req/min)
- ‚úÖ Retry logic con exponential backoff
- ‚úÖ C√°lculo de costos y tokens
- ‚úÖ Prompt engineering (mec√°nico experto con 20+ a√±os de experiencia)
- ‚úÖ Respuesta estructurada en JSON

### 2. Base de Datos
- ‚úÖ Nuevos campos en tabla `alerts`:
  - `llm_diagnosis` - Diagn√≥stico detallado
  - `llm_recommendations` - Array JSON de recomendaciones
  - `llm_severity` - Severidad evaluada por IA
  - `llm_cost` - Costo en USD
  - `llm_tokens` - Tokens utilizados
  - `llm_cached` - Si us√≥ cache de prompt
- ‚úÖ Migraci√≥n SQL creada: `supabase/migrations/0002_add_llm_fields_to_alerts.sql`
- ‚úÖ √çndices para performance

### 3. Integraci√≥n en Simulador (`lib/simulation-manager.ts`)
- ‚úÖ Llamada autom√°tica a LLM al generar alerta
- ‚úÖ Contexto con datos del veh√≠culo (RPM, temp, bater√≠a, etc.)
- ‚úÖ Fallback graceful si LLM falla
- ‚úÖ Guardado de diagn√≥stico en DB

### 4. Interfaz de Usuario
- ‚úÖ Componente `AlertDetailDialog` con dise√±o especializado
- ‚úÖ Visualizaci√≥n de diagn√≥stico del mec√°nico experto
- ‚úÖ Lista de recomendaciones accionables
- ‚úÖ Comparaci√≥n severidad original vs evaluada por IA
- ‚úÖ M√©tricas (tokens, costo, cache status)
- ‚úÖ √çcono Brain (üß†) para alertas con diagn√≥stico IA
- ‚úÖ Dise√±o purple-themed para distinguir contenido IA

## üöÄ Configuraci√≥n Inicial

### Paso 1: Crear archivo `.env`

Copiar `.env.example` a `.env` y configurar:

```bash
cp .env.example .env
```

Editar `.env` y agregar:

```env
# Database (Supabase)
DATABASE_URL=postgresql://postgres:[PASSWORD]@[HOST]:[PORT]/postgres
DIRECT_URL=postgresql://postgres:[PASSWORD]@[HOST]:[PORT]/postgres

# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://[PROJECT].supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=[ANON_KEY]

# OpenAI (REQUERIDO para Fase 1)
OPENAI_API_KEY=sk-proj-[TU_API_KEY]
OPENAI_ORG_ID=org-[TU_ORG_ID]  # Opcional

# Feature Flags
ENABLE_LLM_ALERTS=true
LLM_MODEL=gpt-4o-mini
LLM_MAX_TOKENS=500
LLM_TEMPERATURE=0.3
```

### Paso 2: Aplicar Migraci√≥n a la Base de Datos

**Opci√≥n A: Usando Supabase Dashboard**
1. Ir a https://supabase.com/dashboard/project/[PROJECT]/sql
2. Copiar contenido de `supabase/migrations/0002_add_llm_fields_to_alerts.sql`
3. Pegar y ejecutar en SQL Editor

**Opci√≥n B: Usando Drizzle CLI** (requiere .env configurado)
```bash
npx drizzle-kit push
```

### Paso 3: Instalar Dependencias (si no se hizo)

```bash
npm install
```

### Paso 4: Iniciar el Servidor de Desarrollo

```bash
npm run dev
```

## üß™ C√≥mo Probar la Integraci√≥n

### 1. Iniciar Simulaci√≥n

1. Ir a http://localhost:3000/dashboard
2. Activar el toggle "Simulaci√≥n Activa"
3. Configurar:
   - Intervalo: 10 segundos
   - Duraci√≥n: 60 segundos
   - Veh√≠culos: BUS-001, BUS-002, BUS-003, BUS-004, BUS-005
4. Click en "Iniciar Simulaci√≥n"

### 2. Observar Logs en Consola del Servidor

Deber√≠as ver logs como:

```
[SimulationManager] Vehicle data saved: BUS-001
[OpenAI] Diagnosis generated - $0.0012 (450 tokens)
[SimulationManager] LLM diagnosis generated for BUS-001 - $0.0012
[SimulationManager] Alert created: engine_overheating for BUS-001 (with LLM diagnosis)
```

### 3. Ver Alertas en la UI

1. Ir a "Alertas" en el men√∫ lateral
2. Buscar alertas con √≠cono üß† (Brain) en la descripci√≥n
3. Click en bot√≥n "Ver" de una alerta
4. Verificar que se muestre:
   - Diagn√≥stico detallado del mec√°nico experto
   - Lista de recomendaciones accionables
   - Severidad evaluada por IA
   - M√©tricas (tokens, costo)

### 4. Verificar Base de Datos

```sql
SELECT
  vehicle_id,
  alert_type,
  severity,
  llm_severity,
  llm_diagnosis,
  llm_cost,
  llm_tokens,
  llm_cached
FROM alerts
WHERE llm_diagnosis IS NOT NULL
ORDER BY timestamp DESC
LIMIT 5;
```

## üìä M√©tricas Esperadas

Seg√∫n los criterios de √©xito de Fase 1:

- ‚úÖ **Latencia**: < 3 segundos por diagn√≥stico
- ‚úÖ **Costo**: < $0.01 USD por alerta (usualmente ~$0.001-0.003)
- ‚úÖ **Calidad**: Diagn√≥sticos en espa√±ol, comprensibles y accionables
- ‚úÖ **Fallback**: Si OpenAI falla, alerta se guarda sin diagn√≥stico IA

## üêõ Troubleshooting

### Error: "OPENAI_API_KEY not configured"
- Verificar que `.env` existe y tiene `OPENAI_API_KEY`
- Reiniciar servidor de desarrollo

### Error: "Rate limit exceeded"
- El sistema limita a 50 req/min
- Esperar 1 minuto o ajustar configuraci√≥n en `lib/openai-service.ts`

### No se ven diagn√≥sticos en la UI
- Verificar que migraci√≥n se aplic√≥ correctamente
- Verificar que `ENABLE_LLM_ALERTS=true` en `.env`
- Ver logs del servidor para errores

### Costo muy alto
- Ajustar `LLM_MAX_TOKENS` a un valor menor (ej: 300)
- Verificar que modelo es `gpt-4o-mini` (no `gpt-4` que es m√°s caro)

## üìà Pr√≥ximos Pasos

### Pendientes de Fase 1

- [ ] **1.3 Prompt Caching**: Implementar cache para reducir tokens 50-70%
- [ ] **1.4 Endpoint Manual**: `POST /api/alerts/analyze` para an√°lisis on-demand
- [ ] **1.2 Mejoras**: Agregar datos hist√≥ricos al contexto LLM

### Siguiente Fase: Mapa en Tiempo Real

Ver `ROADMAP.md` Fase 2 para detalles.

## üí∞ Estimaci√≥n de Costos

Con gpt-4o-mini (precios aproximados):
- **Input**: $0.00015 / 1K tokens
- **Output**: $0.0006 / 1K tokens

**Ejemplo real**:
- Prompt t√≠pico: ~200 tokens input
- Respuesta: ~300 tokens output
- **Costo**: ~$0.0012 por alerta

**Con 100 alertas/d√≠a**: ~$0.12/d√≠a = **$3.60/mes**

## üìö Referencias

- OpenAI API Docs: https://platform.openai.com/docs
- Drizzle ORM: https://orm.drizzle.team
- Supabase: https://supabase.com/docs

---

**√öltima actualizaci√≥n**: 2025-01-09
**Autor**: @RenzoLenes
**Fase**: 1 - Alertas Inteligentes con LLM ‚úÖ
